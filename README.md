# cs5780-movie-review

Cornell University - CS 4780/5780 - Machine Learning, Fall 2023

Kaggle Competition on Movie Review Preference Analysis. Binary classification of positive/negative movie reviews for extra credit. Had the highest classification score out of a class size of 488 students.

https://www.kaggle.com/competitions/movie-review-preference-analysis/

Students: Anthony Coffin-Schmitt (awc93) and Hansal Shah (hms262)

Our initial approach was to quickly test out several different vanilla classification models with default parameters.
Any models that showed some initial promise via accuracy scores were than finetuned. We utilized scikit-learn and
Xgboost libraries to try these following models: 
- RandomForestClassifier  (Accuracy: 0.76306 with 100 n_estimators, 0.78053 with 1000 n_estimators)
- xgb.XGBRegressor        (Accuracy: 0.76533)
- xgb.XGBClassifier       (Accuracy: 0.78493 with binary:logistic, 0.7708 with binary:hinge)
- Siamese Neural Network  (Accuracy: 0.82232)
- sklearn.svm SVC         (Accuracy 0.8144 with 'linear' kernel, 0.83293 with 'rbf' kernel)
    
From this starting point, we decided to pursue the SVM classification model as it had a vanilla accuracy score of 
0.83293 with a 'rbf' kernel. Data preperation included doubling the size of the testing data by concatennating the 
train_emb1 and train_emb2 to a size of 37,500 samples. Corresponding labels were generated by inverting the given
labels for train_emb1 and concatennating. We did split and shuffle the data with a 80% training and 20% validation,
however we soon switched to using the built-in cross validation functions provided by scikit-learn.

A logrithmic GridSearchCV was performed on the SVC model with a 'rbf' kernel and 'C': [0.1, 1, 10, 100] and 
'gamma': [0.001, 0.01, 0.1, 1, 10]. This search was discontinued early as it became clear parameters closer to
the default 'C'=1, 'gamma'=1 would be better. A second GridSearchCV was performed with 'C': [0.5, 1, 1.5], 
'gamma': [0.5, 1, 1.5, 3] and resulted in the best parameters of 'C'=1.5 and 'gamma'=3. A model was trained with 
those parameters, giving an accuracy of 0.8416. Because in testing we are given two reviews, we decided in cases
where the paired review predications were the same, to select the prediction based off the review with higher
confidence. Using the svm_classifier.predict_proba function. Our Kaggle submission resulted in an accuracy score
of 0.92067 (submission_2_svm.csv). We than performed a larger GridSearch with more parameters and let that run
overnight. The resulting best params were 'C'=2.5 and 'gamma'=2.25. This model resulted in a Kaggle score of
0.92197 (submission_3_svm.csv).

It did not seem likley that additional parameter tuning would give much higher scores so as we decided to take
advantage of the fact that our model was already scoring high to see if we could increase the training data 
samples. This was done by using our most recent model and filtering our test predictions where both test_emb1
and test_emb2 predictions had high confidence of > 0.9, and also predicted different reivew types. This filtered
test reviews and the corresponding labels were added to our training data and training labels sets. The model was
retrained with the added samples with the same parameters. This high confidence model scored 0.9275 on Kaggle, 
(submission_4_hc_svm.csv), an improvment of 0.00553 accuracy. Adding testing data to training data is definitely
risky as it could introduce incorrect bias if your predications are wrong. We were mostly using this as an
opportunity to explore the data and predictions, but would use this approach with caution.


